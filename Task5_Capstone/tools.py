import os

import openai
from dotenv import load_dotenv

from vector_store import VectorStore

load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("MODEL")

client = openai.OpenAI(api_key=API_KEY)
vector_store = VectorStore()

tools = [
    {
        "type": "function",
        "function": {
            "name": "semantic_search",
            "description": "Search the most relevant information in the specified knowledge base, when the user asks questions.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The user's search question.",
                    }
                },
                "required": ["query"],
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
    {
        "type": "function",
        "function": {
            "name": "summarize_session",
            "description": "Summarize the conversation between the assistant and the user.",
            "parameters": {
                "type": "object",
                "properties": {
                    "messages": {
                        "type": "array",
                        "description": "List of chat messages with roles and content.",
                        "items": {
                            "type": "object",
                            "properties": {
                                "role": {"type": "string"},
                                "content": {"type": "string"},
                            },
                            "required": ["role", "content"],
                            "additionalProperties": False,
                        },
                    }
                },
                "required": ["messages"],
                "additionalProperties": False,
            },
            "strict": True,
        },
    },
]


def call_function(name: str, args: dict[str, str | float]) -> str | float:
    """
    Calls the appropriate function based on the given function name and arguments.

    Args:
        name (str): The name of the function to call.
        args (dict[str, str | float]): Arguments to pass to the function.

    Returns:
        str | float: The result returned by the called function.
    """

    function_dict = {
        "semantic_search": semantic_search,
        "summarize_session": summarize_session,
    }

    return function_dict.get(name)(**args)


def semantic_search(query: str) -> str:
    """
    Semantic search in the vector store and formats the results as a prompt.

    Args:
        query (str): The user's search query.

    Returns:
        str: A formatted string combining relevant sources with the user's query.
    """
    results = vector_store.search(client, query)
    return make_query(query, results)


def make_query(query: str, texts: list[tuple[int, str]]) -> str:
    """
    Generate a GPT query prompt using an introduction, source articles, and a user question.

    Args:
        query (str): The user's input question.
        texts (list[str]): A list of relevant text passages from semantic search.

    Returns:
        str: A formatted prompt string to send to GPT, including sources and instructions for citation.
    """
    introduction = (
        "Below are the most relevant knowledge base articles retrieved via semantic search.\n"
        "Use them to answer the question if they are relevant.\n"
        "Cite your sources using the format 'According to Source #N'.\n"
        "If the articles do not sufficiently answer the question, use your own knowledge "
        "and clearly state that the answer is based on general knowledge, not the provided sources."
    )

    question = f"\n\nQuestion: {query}"
    articles = "\n\n".join([f"Source #{idx} {text}" for idx, text in texts])
    return f"{introduction}\n\n{articles}{question}"


def summarize_session(messages: list[dict]) -> str:
    """
    Generates a summary of the conversation between the user and assistant.

    Args:
        messages (list[dict]): List of chat messages.

    Returns:
        str: A concise summary generated by the assistant.
    """
    conversation_text = "\n".join(
        f"{message['role']}: {message['content']}"
        for message in messages
        if message["role"] in ["user", "assistant"] and message.get("content")
    )

    summary_prompt = [
        {
            "role": "system",
            "content": "You are a helpful assistant that summarizes conversations. Provide a concise summary highlighting only the key points discussed.",
        },
        {
            "role": "user",
            "content": f"Summarize the following conversation:\n\n{conversation_text}",
        },
    ]

    response = client.chat.completions.create(model=MODEL_NAME, messages=summary_prompt)

    return response.choices[0].message.content
