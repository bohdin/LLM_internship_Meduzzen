from typing import AsyncIterator

from langchain.agents import AgentType, initialize_agent
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from langchain.memory import ConversationBufferWindowMemory
from langchain_openai import ChatOpenAI

from tools import tools


class StreamingAgent:
    def __init__(self, model_name: str, api_key: str, memory_k: int = 5):
        self.callback = AsyncIteratorCallbackHandler()
        self.llm = ChatOpenAI(
            model=model_name, api_key=api_key, streaming=True, callbacks=[self.callback]
        )
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="output",
            k=memory_k,
        )
        self.agent = initialize_agent(
            tools=tools,
            llm=self.llm,
            memory=self.memory,
            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
            verbose=False,
            handle_parsing_errors=True,
        )

    async def run_stream(self, user_input: str) -> AsyncIterator[str]:
        """
        Asynchronously yields response chunks as they are generated by the agent.

        Args:
            user_input (str): Input text from the user.

        Yields:
            str: Chunk of the agent's response.
        """
        full_response = ""
        capture = False

        async for event in self.agent.astream_events(
            {"input": user_input}, version="v1"
        ):
            if event["event"] == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                print(f"Chunk from agent: {repr(content)}")
                full_response += content
                if capture:
                    yield content
                elif "AI:" in full_response:
                    capture = True
